{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f600608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check torch\n",
    "%pip install transformers sentence-transformers\n",
    "\n",
    "%pip install -q -U bitsandbytes==0.45.3\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8b1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"question\": \"What is a variable in programming?\",\n",
    "        \"answer\": \"A variable is a named memory location used to store data that can be read or modified during program execution.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between a list and an array?\",\n",
    "        \"answer\": \"A list can grow or shrink dynamically, while an array usually has a fixed size and often stores elements of the same data type.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a for loop used for?\",\n",
    "        \"answer\": \"A for loop is used to repeat a block of code a specific number of times or to iterate over a collection of elements.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a function?\",\n",
    "        \"answer\": \"A function is a reusable block of code designed to perform a specific task, which can accept parameters and optionally return a result.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an API?\",\n",
    "        \"answer\": \"An API (Application Programming Interface) defines rules and methods that allow different software applications to communicate with each other.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does it mean to debug a program?\",\n",
    "        \"answer\": \"Debugging is the process of identifying, analyzing, and fixing errors or unexpected behavior in a program.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an exception?\",\n",
    "        \"answer\": \"An exception is an error or unexpected event that occurs during program execution and can disrupt the normal flow of the program.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Difference between == and === in JavaScript?\",\n",
    "        \"answer\": \"== compares values after performing type conversion, while === compares both value and type without any conversion.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an algorithm?\",\n",
    "        \"answer\": \"An algorithm is a well-defined, step-by-step procedure used to solve a problem or perform a computation.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does open source mean?\",\n",
    "        \"answer\": \"Open source software has source code that is publicly available and can be studied, modified, and distributed by anyone.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an object in OOP?\",\n",
    "        \"answer\": \"An object is an instance of a class that contains data (attributes) and behavior (methods) related to that data.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Git used for?\",\n",
    "        \"answer\": \"Git is a version control system used to track changes in source code and collaborate efficiently with other developers.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an infinite loop?\",\n",
    "        \"answer\": \"An infinite loop is a loop that runs indefinitely because its stopping condition is never met or never changes.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an SQL SELECT query?\",\n",
    "        \"answer\": \"A SELECT query is used to retrieve specific data from one or more tables in a database.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do frontend and backend mean?\",\n",
    "        \"answer\": \"Frontend refers to the user-facing part of an application, while backend handles server-side logic, databases, and application processing.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84451936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a658d4fc0b48b3a44de4820bb49ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad20e6799041b1840fb6a60daddf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526f085db6424b3392f1ee8d0a78df6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c7fbca63540d99169c94fcafbc60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d110260d674340b3b30a149583235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bb20085fdd42c6a04f10174d6abd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d203f764b964c6595593f636376beee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_MODEL device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# MODEL_NAME = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "MODEL_NAME = \"Gensyn/Qwen2.5-1.5B-Instruct\"\n",
    "EMBEDDING_MODEL = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "LLM_MODEL = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "LLM_MODEL.to(DEVICE)\n",
    "_ = LLM_MODEL.eval()\n",
    "\n",
    "# print LLM_MODEL device\n",
    "print(f\"LLM_MODEL device: {next(LLM_MODEL.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ab1132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shots: 0\n",
      "Prompt:\n",
      "QUESTION:\n",
      "What is a for loop used for?\n",
      "ANSWER:\n",
      "\n",
      "Prompt:\n",
      "QUESTION:\n",
      "What is a function?\n",
      "ANSWER:\n",
      "\n",
      "Number of shots: 2\n",
      "Prompt:\n",
      "QUESTION:\n",
      "What is an SQL SELECT query?\n",
      "ANSWER:\n",
      "A SELECT query is used to retrieve specific data from one or more tables in a database.\n",
      "\n",
      "QUESTION:\n",
      "What is an exception?\n",
      "ANSWER:\n",
      "An exception is an error or unexpected event that occurs during program execution and can disrupt the normal flow of the program.\n",
      "\n",
      "QUESTION:\n",
      "What is a for loop used for?\n",
      "ANSWER:\n",
      "\n",
      "Prompt:\n",
      "QUESTION:\n",
      "Difference between == and === in JavaScript?\n",
      "ANSWER:\n",
      "== compares values after performing type conversion, while === compares both value and type without any conversion.\n",
      "\n",
      "QUESTION:\n",
      "What is the difference between a list and an array?\n",
      "ANSWER:\n",
      "A list can grow or shrink dynamically, while an array usually has a fixed size and often stores elements of the same data type.\n",
      "\n",
      "QUESTION:\n",
      "What is a function?\n",
      "ANSWER:\n",
      "\n",
      "Number of shots: 4\n",
      "Prompt:\n",
      "QUESTION:\n",
      "What is an SQL SELECT query?\n",
      "ANSWER:\n",
      "A SELECT query is used to retrieve specific data from one or more tables in a database.\n",
      "\n",
      "QUESTION:\n",
      "What is an exception?\n",
      "ANSWER:\n",
      "An exception is an error or unexpected event that occurs during program execution and can disrupt the normal flow of the program.\n",
      "\n",
      "QUESTION:\n",
      "What is the difference between a list and an array?\n",
      "ANSWER:\n",
      "A list can grow or shrink dynamically, while an array usually has a fixed size and often stores elements of the same data type.\n",
      "\n",
      "QUESTION:\n",
      "What do frontend and backend mean?\n",
      "ANSWER:\n",
      "Frontend refers to the user-facing part of an application, while backend handles server-side logic, databases, and application processing.\n",
      "\n",
      "QUESTION:\n",
      "What is a for loop used for?\n",
      "ANSWER:\n",
      "\n",
      "Prompt:\n",
      "QUESTION:\n",
      "Difference between == and === in JavaScript?\n",
      "ANSWER:\n",
      "== compares values after performing type conversion, while === compares both value and type without any conversion.\n",
      "\n",
      "QUESTION:\n",
      "What is the difference between a list and an array?\n",
      "ANSWER:\n",
      "A list can grow or shrink dynamically, while an array usually has a fixed size and often stores elements of the same data type.\n",
      "\n",
      "QUESTION:\n",
      "What is an algorithm?\n",
      "ANSWER:\n",
      "An algorithm is a well-defined, step-by-step procedure used to solve a problem or perform a computation.\n",
      "\n",
      "QUESTION:\n",
      "What do frontend and backend mean?\n",
      "ANSWER:\n",
      "Frontend refers to the user-facing part of an application, while backend handles server-side logic, databases, and application processing.\n",
      "\n",
      "QUESTION:\n",
      "What is a function?\n",
      "ANSWER:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Simple container for a question/answer pair.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text.\n",
    "    answer : str\n",
    "        The answer text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        A mapping with keys \"question\" and \"answer\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: dict):\n",
    "        self.question = data[\"question\"]\n",
    "        self.answer = data[\"answer\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return a string representation of the Data object.\"\"\"\n",
    "        return f\"Data(question={self.question}, answer={self.answer})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Check equality between two Data objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : Data\n",
    "            Another Data object to compare with.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if both question and answer match, NotImplemented if other is not a Data.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Data):\n",
    "            return NotImplemented\n",
    "        return self.question == other.question and self.answer == other.answer\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Collection of Data objects with convenient accessors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list[dict]\n",
    "        Iterable of dictionaries, each containing \"question\" and \"answer\".\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data : list[Data]\n",
    "        List of Data objects created from input dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: list):\n",
    "        self.data = [Data(entry) for entry in data]\n",
    "\n",
    "    def get_random_entry(self):\n",
    "        \"\"\"Return a single random Data object from the dataset.\"\"\"\n",
    "        return random.choice(self.data)\n",
    "\n",
    "    def get_all_entries(self):\n",
    "        \"\"\"Return the list of all Data objects.\"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def generate_random_entries(\n",
    "        self, count: int = 1, forbiden_entry: list[Data] = []\n",
    "    ) -> list[Data]:\n",
    "        \"\"\"\n",
    "        Generate a list of unique random Data entries, excluding forbidden ones.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count : int, optional (default=1)\n",
    "            Number of random entries to generate.\n",
    "        forbiden_entry : list[Data], optional (default=[])\n",
    "            List of Data entries to exclude from selection.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Data]\n",
    "            A list of randomly selected Data objects.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If count is not between 0 and dataset size minus one.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= count <= len(self.data) - 1\n",
    "        ), \"n must be between 0 and the size of the dataset minus one.\"\n",
    "        other_entries = random.sample(\n",
    "            list(itertools.filterfalse(lambda e: e in forbiden_entry, self.data)),\n",
    "            count,\n",
    "        )\n",
    "        return other_entries\n",
    "\n",
    "    def add_other_entries(\n",
    "        self,\n",
    "        forbiden_entry: list[Data],\n",
    "        other_entries: list[Data],\n",
    "        number_of_shots: int,\n",
    "    ) -> list[Data]:\n",
    "        \"\"\"\n",
    "        Add additional random entries to an existing list, avoiding duplicates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        forbiden_entry : list[Data]\n",
    "            List of Data entries to exclude from selection.\n",
    "        other_entries : list[Data]\n",
    "            Existing list of entries to extend.\n",
    "        number_of_shots : int\n",
    "            Number of additional entries to add.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Data]\n",
    "            Combined list of other_entries plus the additional random entries.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If number_of_shots is not between 0 and dataset size minus one.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= number_of_shots <= len(self.data) - 1\n",
    "        ), \"n must be between 0 and the size of the dataset minus one.\"\n",
    "        additional_entries = random.sample(\n",
    "            list(\n",
    "                itertools.filterfalse(\n",
    "                    lambda e: e in forbiden_entry or e in other_entries, self.data\n",
    "                )\n",
    "            ),\n",
    "            number_of_shots,\n",
    "        )\n",
    "        return other_entries + additional_entries\n",
    "\n",
    "    def build_prompt(self, main_entry: Data, other_entries: list[Data]) -> str:\n",
    "        \"\"\"\n",
    "        Build a formatted prompt string from example entries and a main question.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_entry : Data\n",
    "            The main question/answer pair whose question will be asked.\n",
    "        other_entries : list[Data]\n",
    "            List of example question/answer pairs to include as few-shot examples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Formatted prompt with examples followed by the main question.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If main_entry is not part of the dataset.\n",
    "        \"\"\"\n",
    "        assert main_entry in self.data, \"main_entry must be part of the dataset\"\n",
    "\n",
    "        return (\n",
    "            \"\".join(\n",
    "                f\"QUESTION:\\n{e.question}\\nANSWER:\\n{e.answer}\\n\\n\"\n",
    "                for e in other_entries\n",
    "            )\n",
    "            + f\"QUESTION:\\n{main_entry.question}\\nANSWER:\\n\"\n",
    "        )\n",
    "\n",
    "    def generate_prompt(self, number_of_shots: int = 0) -> tuple[str, Data]:\n",
    "        \"\"\"\n",
    "        Generate a few-shot prompt for the language model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        number_of_shots : int, optional (default=0)\n",
    "            Number of example question/answer pairs to include before the main question.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[str, Data]\n",
    "            A tuple containing:\n",
    "            - The formatted prompt string with examples and the main question.\n",
    "            - The main Data entry whose answer should be generated.\n",
    "        \"\"\"\n",
    "        main_entry = self.get_random_entry()\n",
    "        other_entries = self.generate_random_entries(\n",
    "            count=number_of_shots, forbiden_entry=[main_entry]\n",
    "        )\n",
    "        prompt = self.build_prompt(main_entry, other_entries)\n",
    "        return prompt, main_entry\n",
    "\n",
    "    def generate_multiple_prompts(\n",
    "        self, count=1, number_of_shots=[0]\n",
    "    ) -> dict[int, list[tuple[str, Data]]]:\n",
    "        \"\"\"\n",
    "        Generate multiple few-shot prompts for different shot counts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count : int, optional (default=1)\n",
    "            Number of main entries to generate prompts for.\n",
    "        number_of_shots : list[int], optional (default=[0])\n",
    "            List of shot counts to generate prompts for each main entry.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[int, list[tuple[str, Data]]]\n",
    "            A dictionary mapping each shot count to a list of tuples, where each tuple contains:\n",
    "            - The formatted prompt string with examples and the main question.\n",
    "            - The main Data entry whose answer should be generated.\n",
    "        \"\"\"\n",
    "        number_of_shots = sorted(number_of_shots)\n",
    "        prompts = {}\n",
    "        for shots in number_of_shots:\n",
    "            prompts[shots] = []\n",
    "        main_entries = self.generate_random_entries(count=count)\n",
    "        for main_entry in main_entries:\n",
    "            current_other_entries = []\n",
    "            for shots in number_of_shots:\n",
    "                if shots > 0:\n",
    "                    current_other_entries = self.add_other_entries(\n",
    "                        forbiden_entry=[main_entry],\n",
    "                        other_entries=current_other_entries,\n",
    "                        number_of_shots=shots - len(current_other_entries),\n",
    "                    )\n",
    "                prompt = self.build_prompt(main_entry, current_other_entries)\n",
    "                prompts[shots].append((prompt, main_entry))\n",
    "        return prompts\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Hybrid evaluator for assessing model-generated answers.\n",
    "\n",
    "    The final score combines:\n",
    "    - embedding cosine similarity (semantic correctness)\n",
    "    - keyword recall (content coverage)\n",
    "    - length penalty (conciseness)\n",
    "\n",
    "    The score is normalized between 0.0 and 1.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model : SentenceTransformer\n",
    "        The embedding model used for computing semantic similarity.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model : SentenceTransformer\n",
    "        The embedding model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model):\n",
    "        self.model = embedding_model\n",
    "\n",
    "    def _tokenize(self, text: str) -> set:\n",
    "        \"\"\"\n",
    "        Simple word tokenizer that lowercases and removes punctuation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            The text to tokenize.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        set\n",
    "            A set of lowercase word tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        return set(text.split())\n",
    "\n",
    "    def _keyword_score(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute keyword recall: proportion of reference words present in generated answer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated : str\n",
    "            The generated answer text.\n",
    "        reference : str\n",
    "            The reference answer text.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Recall score between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        ref_tokens = self._tokenize(reference)\n",
    "        gen_tokens = self._tokenize(generated)\n",
    "\n",
    "        if not ref_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        return len(ref_tokens & gen_tokens) / len(ref_tokens)\n",
    "\n",
    "    def _length_score(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Penalize excessive verbosity using a smooth ratio-based score.\n",
    "\n",
    "        Score is 1.0 when lengths match, decreases otherwise.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated : str\n",
    "            The generated answer text.\n",
    "        reference : str\n",
    "            The reference answer text.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Length penalty score between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        gen_len = max(len(generated.split()), 1)\n",
    "        ref_len = max(len(reference.split()), 1)\n",
    "\n",
    "        ratio = gen_len / ref_len\n",
    "\n",
    "        # Symmetric penalty: perfect at ratio=1\n",
    "        return math.exp(-abs(math.log(ratio)))\n",
    "\n",
    "    def evaluate(self, generated_answer: str, reference_answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the hybrid evaluation score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_answer : str\n",
    "            The model-generated answer.\n",
    "        reference_answer : str\n",
    "            The reference/expected answer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Final weighted score combining embedding similarity (50%),\n",
    "            keyword recall (30%), and length penalty (20%).\n",
    "        \"\"\"\n",
    "        # Embedding similarity\n",
    "        emb_gen = self.model.encode(generated_answer, convert_to_tensor=True)\n",
    "        emb_ref = self.model.encode(reference_answer, convert_to_tensor=True)\n",
    "        embedding_score = util.cos_sim(emb_gen, emb_ref).item()\n",
    "\n",
    "        # Keyword recall\n",
    "        # keyword_score = self._keyword_score(generated_answer, reference_answer)\n",
    "\n",
    "        # Length penalty\n",
    "        # length_score = self._length_score(generated_answer, reference_answer)\n",
    "\n",
    "        # Weighted final score\n",
    "        # final_score = 0.5 * embedding_score + 0.3 * keyword_score + 0.2 * length_score\n",
    "        final_score = embedding_score\n",
    "        return float(final_score)\n",
    "\n",
    "\n",
    "evaluator = Evaluator(EMBEDDING_MODEL)\n",
    "ds = Dataset(data=dataset)\n",
    "# print(\n",
    "#     evaluator.evaluate(\n",
    "#         \"J'ai un chats dans ma maison.\",\n",
    "#         \"Je possède un chat à la maison.\",\n",
    "#     )\n",
    "# )\n",
    "res = ds.generate_multiple_prompts(count=2, number_of_shots=[0, 2, 4])\n",
    "for k, v in res.items():\n",
    "    print(f\"Number of shots: {k}\")\n",
    "    for prompt, main_entry in v:\n",
    "        print(\"Prompt:\")\n",
    "        print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ea2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "QUESTION:\n",
      "What is an API?\n",
      "ANSWER:\n",
      "An API (Application Programming Interface) defines rules and methods that allow different software applications to communicate with each other.\n",
      "\n",
      "QUESTION:\n",
      "What is an SQL SELECT query?\n",
      "ANSWER:\n",
      "A SELECT query is used to retrieve specific data from one or more tables in a database.\n",
      "\n",
      "QUESTION:\n",
      "What is a variable in programming?\n",
      "ANSWER:\n",
      "A variable is a named memory location used to store data that can be read or modified during program execution.\n",
      "\n",
      "QUESTION:\n",
      "What does open source mean?\n",
      "ANSWER:\n",
      "\n",
      "Generated: Open source means that the source code of a software application is freely available for anyone to use, modify, and distribute. This allows users to inspect how the software works, improve it, and share their improvements with others. Open-source projects often have active communities where developers collaborate on enhancements and bug fixes.\n",
      "Expected: Open source software has source code that is publicly available and can be studied, modified, and distributed by anyone.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7695186138153076"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LLM:\n",
    "    \"\"\"\n",
    "    Wrapper for tokenizer and model to generate chat responses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer to use for encoding/decoding.\n",
    "    model : AutoModelForCausalLM\n",
    "        The language model for generation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer instance.\n",
    "    model : AutoModelForCausalLM\n",
    "        The language model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        messages,\n",
    "        temperature=0.0,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=128,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a response from the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        messages : str or list\n",
    "            The user message/prompt to send to the model. Can be a string\n",
    "            (which will be wrapped as a user message) or a list of message dicts.\n",
    "        temperature : float, optional (default=0.0)\n",
    "            Controls randomness in generation. Higher values (e.g., 1.5) make\n",
    "            output more random, lower values (e.g., 0.2) make it more deterministic.\n",
    "            When set to 0.0, greedy decoding is used (do_sample=False).\n",
    "        top_k : int, optional (default=50)\n",
    "            Number of highest probability tokens to keep for top-k filtering.\n",
    "        top_p : float, optional (default=0.9)\n",
    "            Nucleus sampling: keeps the smallest set of tokens whose cumulative\n",
    "            probability exceeds top_p.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The generated response with thinking tags removed.\n",
    "        \"\"\"\n",
    "        # Convert string to chat format if needed\n",
    "        if isinstance(messages, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "\n",
    "        # Apply chat template and tokenize\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True,\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        do_sample = True\n",
    "        if temperature == 0.0:\n",
    "            do_sample = False\n",
    "\n",
    "        # Build generation kwargs\n",
    "        gen_kwargs = {\n",
    "            \"do_sample\": do_sample,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "        if do_sample:\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "            gen_kwargs[\"top_k\"] = top_k\n",
    "            gen_kwargs[\"top_p\"] = top_p\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(inputs, **gen_kwargs)\n",
    "        decoded = self.tokenizer.decode(\n",
    "            outputs[0][inputs.shape[-1] :],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        # remove <think>...</think> tags\n",
    "        cleaned = re.sub(r\"<think>.*?</think>\", \"\", decoded, flags=re.DOTALL).strip()\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "llm = LLM(TOKENIZER, LLM_MODEL)\n",
    "prompt, main_entry = ds.generate_prompt(number_of_shots=3)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Generated:\", response)\n",
    "print(\"Expected:\", main_entry.answer)\n",
    "evaluator.evaluate(response, main_entry.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f559f030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations: 100%|██████████| 54/54 [12:09<00:00, 13.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f92d6_row0_col4 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row1_col4 {\n",
       "  background-color: #1b9950;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row2_col4 {\n",
       "  background-color: #279f53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row3_col4 {\n",
       "  background-color: #2da155;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row4_col4 {\n",
       "  background-color: #3ca959;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row5_col4 {\n",
       "  background-color: #42ac5a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row6_col4 {\n",
       "  background-color: #66bd63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row7_col4 {\n",
       "  background-color: #6ec064;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row8_col4 {\n",
       "  background-color: #84ca66;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row9_col4 {\n",
       "  background-color: #9bd469;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row10_col4 {\n",
       "  background-color: #a2d76a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row11_col4 {\n",
       "  background-color: #abdb6d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row12_col4 {\n",
       "  background-color: #afdd70;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row13_col4, #T_f92d6_row14_col4 {\n",
       "  background-color: #b3df72;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row15_col4, #T_f92d6_row16_col4 {\n",
       "  background-color: #b7e075;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row17_col4 {\n",
       "  background-color: #b9e176;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row18_col4 {\n",
       "  background-color: #bde379;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row19_col4, #T_f92d6_row20_col4 {\n",
       "  background-color: #c9e881;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row21_col4 {\n",
       "  background-color: #cdea83;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row22_col4 {\n",
       "  background-color: #d1ec86;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row23_col4 {\n",
       "  background-color: #d3ec87;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row24_col4 {\n",
       "  background-color: #daf08d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row25_col4 {\n",
       "  background-color: #f1f9ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row26_col4 {\n",
       "  background-color: #fafdb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row27_col4 {\n",
       "  background-color: #fbfdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row28_col4 {\n",
       "  background-color: #fff3ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row29_col4 {\n",
       "  background-color: #fff2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row30_col4 {\n",
       "  background-color: #fff1a8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row31_col4 {\n",
       "  background-color: #feefa3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row32_col4 {\n",
       "  background-color: #fdb96a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row33_col4 {\n",
       "  background-color: #fcaa5f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f92d6_row34_col4 {\n",
       "  background-color: #f7814c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row35_col4, #T_f92d6_row36_col4 {\n",
       "  background-color: #d62f27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row37_col4 {\n",
       "  background-color: #d42d27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f92d6_row38_col4 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f92d6\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f92d6_level0_col0\" class=\"col_heading level0 col0\" >number_of_shots</th>\n",
       "      <th id=\"T_f92d6_level0_col1\" class=\"col_heading level0 col1\" >temperature</th>\n",
       "      <th id=\"T_f92d6_level0_col2\" class=\"col_heading level0 col2\" >top_k</th>\n",
       "      <th id=\"T_f92d6_level0_col3\" class=\"col_heading level0 col3\" >top_p</th>\n",
       "      <th id=\"T_f92d6_level0_col4\" class=\"col_heading level0 col4\" >avg_score</th>\n",
       "      <th id=\"T_f92d6_level0_col5\" class=\"col_heading level0 col5\" >min_score</th>\n",
       "      <th id=\"T_f92d6_level0_col6\" class=\"col_heading level0 col6\" >max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row0\" class=\"row_heading level0 row0\" >34</th>\n",
       "      <td id=\"T_f92d6_row0_col0\" class=\"data row0 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row0_col2\" class=\"data row0 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row0_col3\" class=\"data row0 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row0_col4\" class=\"data row0 col4\" >0.876855</td>\n",
       "      <td id=\"T_f92d6_row0_col5\" class=\"data row0 col5\" >0.802863</td>\n",
       "      <td id=\"T_f92d6_row0_col6\" class=\"data row0 col6\" >0.952257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row1\" class=\"row_heading level0 row1\" >15</th>\n",
       "      <td id=\"T_f92d6_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row1_col1\" class=\"data row1 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row1_col2\" class=\"data row1 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row1_col3\" class=\"data row1 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row1_col4\" class=\"data row1 col4\" >0.867095</td>\n",
       "      <td id=\"T_f92d6_row1_col5\" class=\"data row1 col5\" >0.810071</td>\n",
       "      <td id=\"T_f92d6_row1_col6\" class=\"data row1 col6\" >0.944486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row2\" class=\"row_heading level0 row2\" >35</th>\n",
       "      <td id=\"T_f92d6_row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row2_col1\" class=\"data row2 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row2_col2\" class=\"data row2 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row2_col3\" class=\"data row2 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row2_col4\" class=\"data row2 col4\" >0.865729</td>\n",
       "      <td id=\"T_f92d6_row2_col5\" class=\"data row2 col5\" >0.803265</td>\n",
       "      <td id=\"T_f92d6_row2_col6\" class=\"data row2 col6\" >0.927156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row3\" class=\"row_heading level0 row3\" >20</th>\n",
       "      <td id=\"T_f92d6_row3_col0\" class=\"data row3 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row3_col1\" class=\"data row3 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row3_col2\" class=\"data row3 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row3_col3\" class=\"data row3 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row3_col4\" class=\"data row3 col4\" >0.864920</td>\n",
       "      <td id=\"T_f92d6_row3_col5\" class=\"data row3 col5\" >0.793881</td>\n",
       "      <td id=\"T_f92d6_row3_col6\" class=\"data row3 col6\" >0.933544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row4\" class=\"row_heading level0 row4\" >23</th>\n",
       "      <td id=\"T_f92d6_row4_col0\" class=\"data row4 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row4_col2\" class=\"data row4 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row4_col3\" class=\"data row4 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row4_col4\" class=\"data row4 col4\" >0.863209</td>\n",
       "      <td id=\"T_f92d6_row4_col5\" class=\"data row4 col5\" >0.797097</td>\n",
       "      <td id=\"T_f92d6_row4_col6\" class=\"data row4 col6\" >0.919931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row5\" class=\"row_heading level0 row5\" >28</th>\n",
       "      <td id=\"T_f92d6_row5_col0\" class=\"data row5 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row5_col1\" class=\"data row5 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row5_col2\" class=\"data row5 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row5_col3\" class=\"data row5 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row5_col4\" class=\"data row5 col4\" >0.862399</td>\n",
       "      <td id=\"T_f92d6_row5_col5\" class=\"data row5 col5\" >0.779659</td>\n",
       "      <td id=\"T_f92d6_row5_col6\" class=\"data row5 col6\" >0.946969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row6\" class=\"row_heading level0 row6\" >33</th>\n",
       "      <td id=\"T_f92d6_row6_col0\" class=\"data row6 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row6_col2\" class=\"data row6 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row6_col3\" class=\"data row6 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row6_col4\" class=\"data row6 col4\" >0.858069</td>\n",
       "      <td id=\"T_f92d6_row6_col5\" class=\"data row6 col5\" >0.739278</td>\n",
       "      <td id=\"T_f92d6_row6_col6\" class=\"data row6 col6\" >0.938561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row7\" class=\"row_heading level0 row7\" >37</th>\n",
       "      <td id=\"T_f92d6_row7_col0\" class=\"data row7 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row7_col2\" class=\"data row7 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row7_col3\" class=\"data row7 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row7_col4\" class=\"data row7 col4\" >0.856847</td>\n",
       "      <td id=\"T_f92d6_row7_col5\" class=\"data row7 col5\" >0.717493</td>\n",
       "      <td id=\"T_f92d6_row7_col6\" class=\"data row7 col6\" >0.938808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row8\" class=\"row_heading level0 row8\" >26</th>\n",
       "      <td id=\"T_f92d6_row8_col0\" class=\"data row8 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "      <td id=\"T_f92d6_row8_col2\" class=\"data row8 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row8_col3\" class=\"data row8 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row8_col4\" class=\"data row8 col4\" >0.853578</td>\n",
       "      <td id=\"T_f92d6_row8_col5\" class=\"data row8 col5\" >0.733064</td>\n",
       "      <td id=\"T_f92d6_row8_col6\" class=\"data row8 col6\" >0.955262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row9\" class=\"row_heading level0 row9\" >31</th>\n",
       "      <td id=\"T_f92d6_row9_col0\" class=\"data row9 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row9_col1\" class=\"data row9 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row9_col2\" class=\"data row9 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row9_col3\" class=\"data row9 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row9_col4\" class=\"data row9 col4\" >0.850464</td>\n",
       "      <td id=\"T_f92d6_row9_col5\" class=\"data row9 col5\" >0.733231</td>\n",
       "      <td id=\"T_f92d6_row9_col6\" class=\"data row9 col6\" >0.955197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row10\" class=\"row_heading level0 row10\" >19</th>\n",
       "      <td id=\"T_f92d6_row10_col0\" class=\"data row10 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row10_col1\" class=\"data row10 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row10_col2\" class=\"data row10 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row10_col3\" class=\"data row10 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row10_col4\" class=\"data row10 col4\" >0.849240</td>\n",
       "      <td id=\"T_f92d6_row10_col5\" class=\"data row10 col5\" >0.775777</td>\n",
       "      <td id=\"T_f92d6_row10_col6\" class=\"data row10 col6\" >0.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row11\" class=\"row_heading level0 row11\" >16</th>\n",
       "      <td id=\"T_f92d6_row11_col0\" class=\"data row11 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row11_col1\" class=\"data row11 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row11_col2\" class=\"data row11 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row11_col3\" class=\"data row11 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row11_col4\" class=\"data row11 col4\" >0.847931</td>\n",
       "      <td id=\"T_f92d6_row11_col5\" class=\"data row11 col5\" >0.746635</td>\n",
       "      <td id=\"T_f92d6_row11_col6\" class=\"data row11 col6\" >0.916794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row12\" class=\"row_heading level0 row12\" >18</th>\n",
       "      <td id=\"T_f92d6_row12_col0\" class=\"data row12 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row12_col1\" class=\"data row12 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row12_col2\" class=\"data row12 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row12_col3\" class=\"data row12 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row12_col4\" class=\"data row12 col4\" >0.847175</td>\n",
       "      <td id=\"T_f92d6_row12_col5\" class=\"data row12 col5\" >0.780302</td>\n",
       "      <td id=\"T_f92d6_row12_col6\" class=\"data row12 col6\" >0.925549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row13\" class=\"row_heading level0 row13\" >6</th>\n",
       "      <td id=\"T_f92d6_row13_col0\" class=\"data row13 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row13_col1\" class=\"data row13 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row13_col2\" class=\"data row13 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row13_col3\" class=\"data row13 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row13_col4\" class=\"data row13 col4\" >0.846513</td>\n",
       "      <td id=\"T_f92d6_row13_col5\" class=\"data row13 col5\" >0.821344</td>\n",
       "      <td id=\"T_f92d6_row13_col6\" class=\"data row13 col6\" >0.880717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row14\" class=\"row_heading level0 row14\" >32</th>\n",
       "      <td id=\"T_f92d6_row14_col0\" class=\"data row14 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row14_col1\" class=\"data row14 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row14_col2\" class=\"data row14 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row14_col3\" class=\"data row14 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row14_col4\" class=\"data row14 col4\" >0.846359</td>\n",
       "      <td id=\"T_f92d6_row14_col5\" class=\"data row14 col5\" >0.720980</td>\n",
       "      <td id=\"T_f92d6_row14_col6\" class=\"data row14 col6\" >0.958217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row15\" class=\"row_heading level0 row15\" >24</th>\n",
       "      <td id=\"T_f92d6_row15_col0\" class=\"data row15 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row15_col1\" class=\"data row15 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row15_col2\" class=\"data row15 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row15_col3\" class=\"data row15 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row15_col4\" class=\"data row15 col4\" >0.845637</td>\n",
       "      <td id=\"T_f92d6_row15_col5\" class=\"data row15 col5\" >0.743097</td>\n",
       "      <td id=\"T_f92d6_row15_col6\" class=\"data row15 col6\" >0.924075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row16\" class=\"row_heading level0 row16\" >13</th>\n",
       "      <td id=\"T_f92d6_row16_col0\" class=\"data row16 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row16_col1\" class=\"data row16 col1\" >0.000000</td>\n",
       "      <td id=\"T_f92d6_row16_col2\" class=\"data row16 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row16_col3\" class=\"data row16 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row16_col4\" class=\"data row16 col4\" >0.845621</td>\n",
       "      <td id=\"T_f92d6_row16_col5\" class=\"data row16 col5\" >0.726980</td>\n",
       "      <td id=\"T_f92d6_row16_col6\" class=\"data row16 col6\" >0.930786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row17\" class=\"row_heading level0 row17\" >27</th>\n",
       "      <td id=\"T_f92d6_row17_col0\" class=\"data row17 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row17_col1\" class=\"data row17 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row17_col2\" class=\"data row17 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row17_col3\" class=\"data row17 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row17_col4\" class=\"data row17 col4\" >0.845176</td>\n",
       "      <td id=\"T_f92d6_row17_col5\" class=\"data row17 col5\" >0.726659</td>\n",
       "      <td id=\"T_f92d6_row17_col6\" class=\"data row17 col6\" >0.919437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row18\" class=\"row_heading level0 row18\" >14</th>\n",
       "      <td id=\"T_f92d6_row18_col0\" class=\"data row18 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row18_col1\" class=\"data row18 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row18_col2\" class=\"data row18 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row18_col3\" class=\"data row18 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row18_col4\" class=\"data row18 col4\" >0.844465</td>\n",
       "      <td id=\"T_f92d6_row18_col5\" class=\"data row18 col5\" >0.730510</td>\n",
       "      <td id=\"T_f92d6_row18_col6\" class=\"data row18 col6\" >0.905239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row19\" class=\"row_heading level0 row19\" >21</th>\n",
       "      <td id=\"T_f92d6_row19_col0\" class=\"data row19 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row19_col1\" class=\"data row19 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row19_col2\" class=\"data row19 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row19_col3\" class=\"data row19 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row19_col4\" class=\"data row19 col4\" >0.842484</td>\n",
       "      <td id=\"T_f92d6_row19_col5\" class=\"data row19 col5\" >0.748699</td>\n",
       "      <td id=\"T_f92d6_row19_col6\" class=\"data row19 col6\" >0.922255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row20\" class=\"row_heading level0 row20\" >30</th>\n",
       "      <td id=\"T_f92d6_row20_col0\" class=\"data row20 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row20_col1\" class=\"data row20 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row20_col2\" class=\"data row20 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row20_col3\" class=\"data row20 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row20_col4\" class=\"data row20 col4\" >0.842312</td>\n",
       "      <td id=\"T_f92d6_row20_col5\" class=\"data row20 col5\" >0.708376</td>\n",
       "      <td id=\"T_f92d6_row20_col6\" class=\"data row20 col6\" >0.937636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row21\" class=\"row_heading level0 row21\" >17</th>\n",
       "      <td id=\"T_f92d6_row21_col0\" class=\"data row21 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row21_col1\" class=\"data row21 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row21_col2\" class=\"data row21 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row21_col3\" class=\"data row21 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row21_col4\" class=\"data row21 col4\" >0.841810</td>\n",
       "      <td id=\"T_f92d6_row21_col5\" class=\"data row21 col5\" >0.749581</td>\n",
       "      <td id=\"T_f92d6_row21_col6\" class=\"data row21 col6\" >0.941149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_f92d6_row22_col0\" class=\"data row22 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row22_col1\" class=\"data row22 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row22_col2\" class=\"data row22 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row22_col3\" class=\"data row22 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row22_col4\" class=\"data row22 col4\" >0.841160</td>\n",
       "      <td id=\"T_f92d6_row22_col5\" class=\"data row22 col5\" >0.801969</td>\n",
       "      <td id=\"T_f92d6_row22_col6\" class=\"data row22 col6\" >0.893626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row23\" class=\"row_heading level0 row23\" >25</th>\n",
       "      <td id=\"T_f92d6_row23_col0\" class=\"data row23 col0\" >2</td>\n",
       "      <td id=\"T_f92d6_row23_col1\" class=\"data row23 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row23_col2\" class=\"data row23 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row23_col3\" class=\"data row23 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row23_col4\" class=\"data row23 col4\" >0.840621</td>\n",
       "      <td id=\"T_f92d6_row23_col5\" class=\"data row23 col5\" >0.794234</td>\n",
       "      <td id=\"T_f92d6_row23_col6\" class=\"data row23 col6\" >0.909477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row24\" class=\"row_heading level0 row24\" >29</th>\n",
       "      <td id=\"T_f92d6_row24_col0\" class=\"data row24 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row24_col1\" class=\"data row24 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row24_col2\" class=\"data row24 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row24_col3\" class=\"data row24 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row24_col4\" class=\"data row24 col4\" >0.839251</td>\n",
       "      <td id=\"T_f92d6_row24_col5\" class=\"data row24 col5\" >0.724625</td>\n",
       "      <td id=\"T_f92d6_row24_col6\" class=\"data row24 col6\" >0.914496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row25\" class=\"row_heading level0 row25\" >11</th>\n",
       "      <td id=\"T_f92d6_row25_col0\" class=\"data row25 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row25_col1\" class=\"data row25 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row25_col2\" class=\"data row25 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row25_col3\" class=\"data row25 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row25_col4\" class=\"data row25 col4\" >0.833870</td>\n",
       "      <td id=\"T_f92d6_row25_col5\" class=\"data row25 col5\" >0.798536</td>\n",
       "      <td id=\"T_f92d6_row25_col6\" class=\"data row25 col6\" >0.864567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row26\" class=\"row_heading level0 row26\" >2</th>\n",
       "      <td id=\"T_f92d6_row26_col0\" class=\"data row26 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row26_col1\" class=\"data row26 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row26_col2\" class=\"data row26 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row26_col3\" class=\"data row26 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row26_col4\" class=\"data row26 col4\" >0.831375</td>\n",
       "      <td id=\"T_f92d6_row26_col5\" class=\"data row26 col5\" >0.751362</td>\n",
       "      <td id=\"T_f92d6_row26_col6\" class=\"data row26 col6\" >0.877760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row27\" class=\"row_heading level0 row27\" >7</th>\n",
       "      <td id=\"T_f92d6_row27_col0\" class=\"data row27 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row27_col1\" class=\"data row27 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row27_col2\" class=\"data row27 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row27_col3\" class=\"data row27 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row27_col4\" class=\"data row27 col4\" >0.831291</td>\n",
       "      <td id=\"T_f92d6_row27_col5\" class=\"data row27 col5\" >0.801873</td>\n",
       "      <td id=\"T_f92d6_row27_col6\" class=\"data row27 col6\" >0.870267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row28\" class=\"row_heading level0 row28\" >0</th>\n",
       "      <td id=\"T_f92d6_row28_col0\" class=\"data row28 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row28_col1\" class=\"data row28 col1\" >0.000000</td>\n",
       "      <td id=\"T_f92d6_row28_col2\" class=\"data row28 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row28_col3\" class=\"data row28 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row28_col4\" class=\"data row28 col4\" >0.826702</td>\n",
       "      <td id=\"T_f92d6_row28_col5\" class=\"data row28 col5\" >0.772686</td>\n",
       "      <td id=\"T_f92d6_row28_col6\" class=\"data row28 col6\" >0.848113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row29\" class=\"row_heading level0 row29\" >4</th>\n",
       "      <td id=\"T_f92d6_row29_col0\" class=\"data row29 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row29_col1\" class=\"data row29 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row29_col2\" class=\"data row29 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row29_col3\" class=\"data row29 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row29_col4\" class=\"data row29 col4\" >0.826370</td>\n",
       "      <td id=\"T_f92d6_row29_col5\" class=\"data row29 col5\" >0.728280</td>\n",
       "      <td id=\"T_f92d6_row29_col6\" class=\"data row29 col6\" >0.887890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row30\" class=\"row_heading level0 row30\" >5</th>\n",
       "      <td id=\"T_f92d6_row30_col0\" class=\"data row30 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row30_col1\" class=\"data row30 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row30_col2\" class=\"data row30 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row30_col3\" class=\"data row30 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row30_col4\" class=\"data row30 col4\" >0.825903</td>\n",
       "      <td id=\"T_f92d6_row30_col5\" class=\"data row30 col5\" >0.750684</td>\n",
       "      <td id=\"T_f92d6_row30_col6\" class=\"data row30 col6\" >0.885968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row31\" class=\"row_heading level0 row31\" >10</th>\n",
       "      <td id=\"T_f92d6_row31_col0\" class=\"data row31 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row31_col1\" class=\"data row31 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row31_col2\" class=\"data row31 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row31_col3\" class=\"data row31 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row31_col4\" class=\"data row31 col4\" >0.825159</td>\n",
       "      <td id=\"T_f92d6_row31_col5\" class=\"data row31 col5\" >0.754170</td>\n",
       "      <td id=\"T_f92d6_row31_col6\" class=\"data row31 col6\" >0.882068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row32\" class=\"row_heading level0 row32\" >1</th>\n",
       "      <td id=\"T_f92d6_row32_col0\" class=\"data row32 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row32_col1\" class=\"data row32 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row32_col2\" class=\"data row32 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row32_col3\" class=\"data row32 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row32_col4\" class=\"data row32 col4\" >0.813793</td>\n",
       "      <td id=\"T_f92d6_row32_col5\" class=\"data row32 col5\" >0.732390</td>\n",
       "      <td id=\"T_f92d6_row32_col6\" class=\"data row32 col6\" >0.898083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row33\" class=\"row_heading level0 row33\" >9</th>\n",
       "      <td id=\"T_f92d6_row33_col0\" class=\"data row33 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row33_col1\" class=\"data row33 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row33_col2\" class=\"data row33 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row33_col3\" class=\"data row33 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row33_col4\" class=\"data row33 col4\" >0.811073</td>\n",
       "      <td id=\"T_f92d6_row33_col5\" class=\"data row33 col5\" >0.736507</td>\n",
       "      <td id=\"T_f92d6_row33_col6\" class=\"data row33 col6\" >0.869063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row34\" class=\"row_heading level0 row34\" >3</th>\n",
       "      <td id=\"T_f92d6_row34_col0\" class=\"data row34 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row34_col1\" class=\"data row34 col1\" >0.500000</td>\n",
       "      <td id=\"T_f92d6_row34_col2\" class=\"data row34 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row34_col3\" class=\"data row34 col3\" >0.700000</td>\n",
       "      <td id=\"T_f92d6_row34_col4\" class=\"data row34 col4\" >0.805467</td>\n",
       "      <td id=\"T_f92d6_row34_col5\" class=\"data row34 col5\" >0.674769</td>\n",
       "      <td id=\"T_f92d6_row34_col6\" class=\"data row34 col6\" >0.858074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row35\" class=\"row_heading level0 row35\" >8</th>\n",
       "      <td id=\"T_f92d6_row35_col0\" class=\"data row35 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row35_col1\" class=\"data row35 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row35_col2\" class=\"data row35 col2\" >10</td>\n",
       "      <td id=\"T_f92d6_row35_col3\" class=\"data row35 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row35_col4\" class=\"data row35 col4\" >0.793064</td>\n",
       "      <td id=\"T_f92d6_row35_col5\" class=\"data row35 col5\" >0.732292</td>\n",
       "      <td id=\"T_f92d6_row35_col6\" class=\"data row35 col6\" >0.850672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_f92d6_row36_col0\" class=\"data row36 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row36_col1\" class=\"data row36 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row36_col2\" class=\"data row36 col2\" >50</td>\n",
       "      <td id=\"T_f92d6_row36_col3\" class=\"data row36 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row36_col4\" class=\"data row36 col4\" >0.792889</td>\n",
       "      <td id=\"T_f92d6_row36_col5\" class=\"data row36 col5\" >0.586348</td>\n",
       "      <td id=\"T_f92d6_row36_col6\" class=\"data row36 col6\" >0.957049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row37\" class=\"row_heading level0 row37\" >38</th>\n",
       "      <td id=\"T_f92d6_row37_col0\" class=\"data row37 col0\" >4</td>\n",
       "      <td id=\"T_f92d6_row37_col1\" class=\"data row37 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row37_col2\" class=\"data row37 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row37_col3\" class=\"data row37 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row37_col4\" class=\"data row37 col4\" >0.792480</td>\n",
       "      <td id=\"T_f92d6_row37_col5\" class=\"data row37 col5\" >0.656193</td>\n",
       "      <td id=\"T_f92d6_row37_col6\" class=\"data row37 col6\" >0.942581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f92d6_level0_row38\" class=\"row_heading level0 row38\" >12</th>\n",
       "      <td id=\"T_f92d6_row38_col0\" class=\"data row38 col0\" >0</td>\n",
       "      <td id=\"T_f92d6_row38_col1\" class=\"data row38 col1\" >1.000000</td>\n",
       "      <td id=\"T_f92d6_row38_col2\" class=\"data row38 col2\" >100</td>\n",
       "      <td id=\"T_f92d6_row38_col3\" class=\"data row38 col3\" >0.950000</td>\n",
       "      <td id=\"T_f92d6_row38_col4\" class=\"data row38 col4\" >0.783636</td>\n",
       "      <td id=\"T_f92d6_row38_col5\" class=\"data row38 col5\" >0.653111</td>\n",
       "      <td id=\"T_f92d6_row38_col6\" class=\"data row38 col6\" >0.853659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7850089b1040>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline to test different invoke parameters\n",
    "class ParameterPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline to evaluate LLM performance across different generation parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    llm : LLM\n",
    "        The language model wrapper.\n",
    "    evaluator : Evaluator\n",
    "        The evaluator for scoring answers.\n",
    "    dataset : Dataset\n",
    "        The dataset containing question/answer pairs.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    llm : LLM\n",
    "        The language model wrapper instance.\n",
    "    evaluator : Evaluator\n",
    "        The evaluator instance.\n",
    "    dataset : Dataset\n",
    "        The dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: LLM, evaluator: Evaluator, dataset: Dataset):\n",
    "        self.llm = llm\n",
    "        self.evaluator = evaluator\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def run_experiment(\n",
    "        self,\n",
    "        param_grid: dict,\n",
    "        num_samples: int = 5,\n",
    "        seed: int = 42,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run experiments with different parameter combinations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param_grid : dict\n",
    "            Dictionary where keys are parameter names and values are lists of values to test.\n",
    "            Can include \"number_of_shots\" to vary prompt examples.\n",
    "            Example: {\"temperature\": [0.5, 1.0], \"top_k\": [10, 50], \"number_of_shots\": [0, 2]}\n",
    "        num_samples : int, optional (default=5)\n",
    "            Number of question/answer pairs to use for averaging.\n",
    "        seed : int, optional (default=42)\n",
    "            Random seed for reproducibility.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Results table with columns: number_of_shots, parameter values,\n",
    "            avg_score, min_score, max_score.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        For temperature=0.0, duplicate combinations with same number_of_shots are skipped\n",
    "        since greedy decoding produces deterministic outputs.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Extract number_of_shots from param_grid if present\n",
    "        number_of_shots_values = param_grid.pop(\"number_of_shots\", [0])\n",
    "\n",
    "        # Pre-generate prompts and entries for all number_of_shots values at once\n",
    "        questions = self.dataset.generate_multiple_prompts(\n",
    "            count=num_samples, number_of_shots=number_of_shots_values\n",
    "        )\n",
    "\n",
    "        # Generate all parameter combinations (excluding number_of_shots)\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = list(param_grid.values())\n",
    "        invoke_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "        # Create full combinations including number_of_shots\n",
    "        all_combinations = list(\n",
    "            itertools.product(number_of_shots_values, invoke_combinations)\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        # Track (number_of_shots, top_k) pairs already run for temperature 0\n",
    "        seen_temp0_combinations = set()\n",
    "\n",
    "        for number_of_shots, invoke_combo in tqdm(\n",
    "            all_combinations, desc=\"Testing parameter combinations\"\n",
    "        ):\n",
    "            invoke_params = dict(zip(param_names, invoke_combo))\n",
    "\n",
    "            # For temperature 0, skip top_k and top_p variation for same number_of_shots\n",
    "            if invoke_params.get(\"temperature\", 1.0) == 0.0:\n",
    "                key = number_of_shots\n",
    "                if key in seen_temp0_combinations:\n",
    "                    continue\n",
    "                seen_temp0_combinations.add(key)\n",
    "\n",
    "            scores = []\n",
    "\n",
    "            # Use pre-built prompts and entries\n",
    "            for prompt, main_entry in questions[number_of_shots]:\n",
    "                # Generate response with current invoke parameters\n",
    "                response = self.llm.invoke(prompt, **invoke_params)\n",
    "\n",
    "                # Evaluate response\n",
    "                score = self.evaluator.evaluate(response, main_entry.answer)\n",
    "                scores.append(score)\n",
    "\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "\n",
    "            result = {\n",
    "                \"number_of_shots\": number_of_shots,\n",
    "                **invoke_params,\n",
    "                \"avg_score\": avg_score,\n",
    "                \"min_score\": min(scores),\n",
    "                \"max_score\": max(scores),\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def display_results(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Display results sorted by average score with color gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Results DataFrame from run_experiment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.io.formats.style.Styler\n",
    "            Styled DataFrame with rows sorted by avg_score (descending)\n",
    "            and a red-yellow-green background gradient on the avg_score column.\n",
    "        \"\"\"\n",
    "        df_sorted = df.sort_values(\"avg_score\", ascending=False)\n",
    "        return df_sorted.style.background_gradient(subset=[\"avg_score\"], cmap=\"RdYlGn\")\n",
    "\n",
    "\n",
    "# Define parameter grid to test (including number_of_shots)\n",
    "param_grid = {\n",
    "    \"number_of_shots\": [0, 2, 4],\n",
    "    \"temperature\": [0.0, 0.5, 1.0],\n",
    "    \"top_k\": [10, 50, 100],\n",
    "    \"top_p\": [0.7, 0.95],\n",
    "}\n",
    "\n",
    "# Create pipeline and run experiments\n",
    "pipeline = ParameterPipeline(llm, evaluator, ds)\n",
    "\n",
    "results_df = pipeline.run_experiment(\n",
    "    param_grid=param_grid,\n",
    "    num_samples=5,  # Use 5 question/answer pairs\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Display results table\n",
    "pipeline.display_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
